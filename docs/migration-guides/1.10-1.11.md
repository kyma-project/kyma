# Migrate from v1.10 to v1.11

>**NOTE:** Follow this guide if you want to upgrade from v1.10 to v1.11. The instructions are not mandatory if you already have Kyma v1.11 installed and plan to upgrade to v1.11.1.

## Installation

### IP address assignment for the load balancer

Following a successful Kyma upgrade, it may happen that the load balancer for the `apiserver-proxy` or `istio-ingressgateway` component receives a new external IP address. In such a case, you must add an entry in the DNS server to assign this IP address to a domain. 

>**NOTE:** This issue does not affect Gardener clusters.

Follow these steps:

1. Check the load balancer's IP address. See the example for `apiserver-proxy`:

```bash
kubectl get svc -n kyma-system apiserver-proxy-ssl  -o jsonpath="{.status.loadBalancer.ingress[0].ip}"
```
2.  Verify if the IP address listed in the output matches the existing DNS configuration. If not, update the DNS configuration using the instructions in [this](https://kyma-project.io/docs/#installation-install-kyma-with-your-own-domain-configure-dns-for-the-cluster-load-balancer) document.

### Certificates for cluster provisioning handled by Gardener

We have unified the way Gardener handles certificates required for cluster provisioning. Before 1.11, `apiserver-proxy` still had certificates issued under the xip.io domain with a self-signed certificate, whereas Gardener already managed other certificates. To simplify certificate management, we unified this approach to allow Gardener to manage all certificates. We have also modified TLS certificate handling to ensure Kyma components react to the TLS rotation Gardener provides. 
Bear in mind that the domain for which you request the certificate is subject to character restrictions. This means that if the domain name in the cluster override exceeds 54 characters, installation will fail with following error:

```bash
Step error:  Details: Helm install error: rpc error: code = Unknown desc = Job failed: BackoffLimitExceeded
```
This happens because the certificate issued for `apiserver-proxy` follows the `apiserver.$DOMAIN` naming convention which does not allow this string to exceed 64 characters. 
For example, if your cluster domain is `abcdefghij.myproject1.shoot.canary.k8s-hana.ondemand.com`, it already has 56 characters. Together with the `apiserver` prefix, it gives a total of 66 characters which results in an error.
To avoid any potential issues with the upgrade, make sure your existing cluster domain name does not exceed 54 characters. Run:

```bash
kubectl get cm -n kyma-installer net-global-overrides -o jsonpath="{.data['global\.\domainName']}" | wc -c
```
If the output value exceeds 54 characters, the installation of `apiserver-proxy` will fail. In such a case, you must create a new cluster with a shorter name.
For details on character restrictions, see [this](https://gardener.cloud/050-tutorials/content/howto/x509_certificates/#character-restrictions) document.


## CLI

In this release, we introduced changes to the Kyma installation process to avoid potential issues during the Kyma upgrade. We ensured compliance with the recent Kyma CLI version, but using older versions results in the following error displayed during Kyma installation:

```bash 
- Kyma Installer deployed 
X Configuring Helm 
Error: jobs.batch "helm-certs-job" not found 
``` 
To ensure error-free Kyma installation, [upgrade](https://github.com/kyma-project/cli#installation) the Kyma CLI to the latest version.

## Knative Eventing Mesh 

To use the new [Knative Eventing Mesh](https://kyma-project.io/docs/master/components/knative-eventing-mesh/#overview-overview), you must upgrade Kyma from version 1.10 to version 1.11. To do so, follow the [Kyma upgrade guide](https://kyma-project.io/docs/#installation-upgrade-kyma). 

>**CAUTION:** Events sent during the migration can be lost.
    
### Verify migration

Perform the following steps to see if the migration process was successful.

1. List all the applications:

    ```bash
     kubectl get applications 
    ```  
2. Check if each Application has a corresponding HTTP Source Adapter:

    ```bash
    kubectl -n kyma-integration get httpsource
    ```
3. Check if the Channel status is `READY`:

    ```bash
    kubectl -n kyma-integration get channels
   ```
4. Check if Kyma Subscriptions have been converted to Knative Triggers:

    ```bash
    kubectl -n {NAMESPACE} get triggers 
    ``` 
    >**NOTE:** If any of the steps results in a failure, recreate the Application using the Kyma Console. If the problem persists, create a support ticket.

5. If you added a service using an EventsServiceClass, check if the service instance is present in your Namespace:
    
    ```bash
    kubectl -n {NAMESPACE} get serviceinstances.servicecatalog.k8s.io 
    ```
  
6. Check if a Broker exists in your Namespace:
    
    ```bash
    kubectl -n {NAMESPACE} get brokers
    ```
7. Check of the Eventing Mesh is enabled for your Namespace:
    ```bash
    kubectl get ns -lknative-eventing-injection=enabled
    ```
    
8. Check if a Knative Subscription exists in the `kyma-integration` Namespace, linking the Knative Channel to the Knative Broker: 

    ```bash
    kubectl -n kyma-integration get subscriptions.messaging.knative.dev 
    ```
9. Check if a subscriber URI in the Knative Subscription points to the Broker:

    ```bash
    kubectl -n kyma-integration get subscriptions.messaging.knative.dev -o jsonpath='{ .items[*].spec.subscriber.uri }' -lapplication-name 
    ```
    >**NOTE:** If any of the steps results in a failure, delete the service instance and recreate it using the Kyma  Console. If the problem persists, create a support ticket.
    
### Eventing endpoints

After the migration, use the new Knative Eventing Mesh endpoint, `https://gateway.domain/APP_NAME/events`, to send events.

>**NOTE:** The deprecated `https://gateway.domain/APP_NAME/v1/events` endpoint will be handled by the compatibility layer which uses the new Eventing Mesh to route events to the sinks.

### Verify event processing

After the Kyma upgrade, there can be issues with event processing that are caused by Subscriptions not being removed and recreated properly. To fix this, perform the following steps:

1. Get all Triggers in the cluster:

```bash
kubectl get triggers.eventing.knative.dev -A
```

If an issue occurs, Triggers will have the **READY** parameter set to `False`. See the example:

```bash
NAMESPACE   NAME                                   READY   REASON                 BROKER    SUBSCRIBER_URI           AGE
test        654331ad-f6ab-56ab-bd91-e0b397afb7b8   False   NotSubscribed          default   http://test.test:8080/   39h
```

3. Get the Trigger's details:

```bash
kubectl describe triggers.eventing.knative.dev -n {NAMESPACE} {TRIGGER_NAME}
```
4. If the status of the Trigger is `Type:Ready Status:False` with the `object is being deleted: subscriptions.messaging.knative.dev` message, get the Subscriptions for the Triggers:

```bash
kubectl get subscriptions.messaging.knative.dev -n {NAMESPACE} -l  eventing.knative.dev/trigger={TRIGGER_NAME}
```

5. Scale `eventing-controller` down:

```bash
kubectl scale deploy -n knative-eventing eventing-controller --replicas=0
```

6. Patch the Subscriptions:

```bash
kubectl patch -n {NAMESPACE} subscriptions.messaging.knative.dev {SUBSCRIPTION_NAME} --type merge -p '{"metadata": {"finalizers": []}}'
```

7. Scale `eventing-controller` up:

```bash
kubectl scale deploy -n knative-eventing eventing-controller --replicas=1
```

Deleting the finalizer makes the garbage collector remove existing Subscriptions and triggers the creation of new Subscriptions. As a result, the link between Triggers and Subscriptions is restored.

To confirm that, check the status of Triggers:

```bash
kubectl get triggers.eventing.knative.dev -n {NAMESPACE} {TRIGGER_NAME}
```