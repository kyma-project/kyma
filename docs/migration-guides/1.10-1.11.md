# Migrate from v1.10 to v1.11

## Kyma upgrade

In this release, we introduced changes to the upgrade procedure. 

>**NOTE:** Following this upgrade guide is enough to start working with Kyma in version 1.11. **Do not** perform the upgrade steps listed on the [Kyma website](https://kyma-project.io/docs/root/kyma/#installation-upgrade-kyma-upgrade-kyma-to-a-newer-version) document.

To upgrade Kyma to version 1.11, follow these steps:

1. Delete the existing `kyma-installer` deployment.

```bash
kubectl delete deployment kyma-installer -n kyma-installer
```
2. Once the deployment is deleted, upgrade Tiller:

```bash
kubectl apply -f https://raw.githubusercontent.com/kyma-project/kyma/{NEW_KYMA_VERSION}/installation/resources/tiller.yaml
```
3. Verify that Tiller is running:

```bash
kubectl get po -n kube-system -l name=tiller -w
```

4. Apply the release file to the cluster to trigger the upgrade:
```bash
kubectl apply -f {KYMA-INSTALLER-CLUSTER-FILE}
```

5. To watch the upgrade process, run:

```bash
while true; do \
kubectl -n default get installation/kyma-installation -o jsonpath="{'Status: '}{.status.state}{', description: '}{.status.description}"; echo; \
sleep 5; \
done
```
### Post-upgrade actions 

After a successful ugprade it may happen that the LoadBalancer for `apiserver-proxy` or `istio-ingressgateway` component receives a new external IP address. In such a case, you must assign this IP address in the DNS server. Follow these steps:

1. Check the LoadBalancer IP. See the example for `apiserver-proxy`:

```bash
kubectl get svc -n kyma-system apiserver-proxy-ssl  -o jsonpath="{.status.loadBalancer.ingress[0].ip}"
```
2. Verify if the output IP address matches exsiting DNS configuration. If not, update the DNS configuration using the instructions in [this](https://kyma-project.io/docs/#installation-install-kyma-with-your-own-domain-configure-dns-for-the-cluster-load-balancer) document.

## Clusters provisioned on Gardener

We have unified the way Gardener handles certificates required for cluster provisioning. Before 1.11, `apiserver-proxy` still had certificates issued under the xip.io domain with a self-signed certificate, whereas Gardener already managed other certificates. To simplify certificate management, we unified this approach to allow Gardener to manage all certificates. We have also modified TLS certificate handling to ensure Kyma components react to the TLS rotation Gardener provides. 
Bear in mind that the domain for which you request the certificate is subject to character restrictions. This means that if the domain name in the cluster override exceeds 54 characters, installation will fail with following error:

```bash
Step error:  Details: Helm install error: rpc error: code = Unknown desc = Job failed: BackoffLimitExceeded
```
This happens because the certificate issued for `apiserver-proxy` follows the `apiserver.$DOMAIN` naming convention which does not allow this string to exceed 64 characters. 
For example, if your cluster domain is `abcdefghij.myproject1.shoot.canary.k8s-hana.ondemand.com`, it already has 56 characters. Together with the `apiserver` prefix, it gives a total of 66 characters which results in an error.
To avoid any potential issues with the upgrade, make sure your existing cluster domain name does not exceed 54 characters. Run:

```bash
kubectl get cm -n kyma-installer net-global-overrides -o jsonpath="{.data['global\.\domainName']}" | wc -c
```
If the output value exceeds 54 characters, the installation of `apiserver-proxy` will fail. In such a case, you must create a new cluster with a shorter name.
For details on character restrictions, see [this](https://gardener.cloud/050-tutorials/content/howto/x509_certificates/#character-restrictions) document.


## CLI

In this release, we introduced changes to the Kyma installation process to avoid potential issues during the Kyma upgrade. We ensured compliance with the recent Kyma CLI version, but using older versions results in the following error displayed during Kyma installation:

```bash 
- Kyma Installer deployed 
X Configuring Helm 
Error: jobs.batch "helm-certs-job" not found 
``` 
To ensure error-free Kyma installation, [upgrade](https://github.com/kyma-project/cli#installation) the Kyma CLI to the latest version.

## Knative Eventing Mesh 

To use the new [Knative Eventing Mesh](https://kyma-project.io/docs/master/components/knative-eventing-mesh/#overview-overview), you must upgrade Kyma from version 1.10 to version 1.11. To do so, follow the [Kyma upgrade guide](https://kyma-project.io/docs/#installation-upgrade-kyma). 

**CAUTION**: Events sent during the migration can be lost.
    
### Verify migration

Perform the following steps to see if the migration process was successful.

1. List all the applications:

    ```bash
     kubectl get applications 
    ```  
2. Check if each Application has a corresponding HTTP Source Adapter. 

    ```bash
    kubectl -n kyma-integration get httpsource
    ```
3. Check if the Channel status is `READY`
    ```bash
    kubectl -n kyma-integration get channels
   ```
4. Check if Kyma Subscriptions have been converted to Knative Triggers:

    ```bash
    kubectl -n <YOUR_NAMESPACE> get triggers 
    ``` 
    **NOTE**: If any of the steps results in a failure, recreate the Application using the Kyma Console. If the           problem persists, create a support ticket.

5. If you added a service using an EventsServiceClass, check if the service instance is present in your Namespace:
    
    ```bash
    kubectl -n <YOUR_NAMESPACE> get serviceinstances.servicecatalog.k8s.io 
    ```
  
6. Check if a Broker exists in your Namespace:
    
    ```bash
    kubectl -n <YOUR_NAMESPACE> get brokers
    ```
7. Check of the Eventing Mesh is enabled for your Namespace:
    ```bash
    kubectl get ns -lknative-eventing-injection=enabled
    ```
    
8. Check if a Knative Subscription exists in the `kyma-integration` Namespace, linking the Knative Channel to the Knative        Broker: 

    ```bash
    kubectl -n kyma-integration get subscriptions.messaging.knative.dev 
    ```
9. Check if a subscriber URI in the Knative Subscription points to the Broker:

    ```bash
    kubectl -n kyma-integration get subscriptions.messaging.knative.dev -o jsonpath='{ .items[*].spec.subscriber.uri }' -lapplication-name 
    ```
    **NOTE**: If any of the steps results in a failure, delete the service instance and recreate it using the Kyma  Console. If the problem persists, create a support ticket.
    
### Eventing endpoints

After the migration, use the new Knative Eventing Mesh endpoint, `https://gateway.domain/APP_NAME/events`, to send events.

**NOTE**: The old endpoint, `https://gateway.domain/APP_NAME/v1/events`, will be handled by the compatibility layer which uses the new Eventing Mesh to route events from the old endpoints to the sinks.

### Verify event flow

It may happen that the eventing flow will not work properly after the upgrade. In such a case, perform the following steps:

1. Get all the Triggers in the cluster:

```bash
kubectl get triggers.eventing.knative.dev -A
```

2. Check if Triggers are ready. See the example:

```bash
NAMESPACE   NAME                                   READY   REASON                 BROKER    SUBSCRIBER_URI           AGE
test        654331ad-f6ab-56ab-bd91-e0b397afb7b8   False   NotSubscribed          default   http://test.test:8080/   39h
```

3. Get more information about the Trigger:

```bash
kubectl describe triggers.eventing.knative.dev -n {namespace} {trigger_name}
```
4. If the status of the trigger is `Type:Ready Status:False` with a message `object is being deleted: subscriptions.messaging.knative.dev`, get the Subscriptions for the Triggers:

```bash
kubectl get subscriptions.messaging.knative.dev -n {namespace} -l  eventing.knative.dev/trigger=<trigger_name>
```

5. Scale down the `eventing-controller`:

```bash
kubectl scale deploy -n knative-eventing eventing-controller --replicas=0
```

6.Patch the Subscriptions:

```bash
kubectl patch -n <namespace> subscriptions.messaging.knative.dev {subscription_name} --type merge -p '{"metadata": {"finalizers": []}}'
```

7. Scale up the `eventing-controller`:

```bash
kubectl scale deploy -n knative-eventing eventing-controller --replicas=1
```

Removing the finalizer will garbage-collect the old Subscriptions and new Subscriptions will be created. This way, Triggers will receive the `Ready` status. Check the status of triggers again.

```bash
kubectl get triggers.eventing.knative.dev -n {namespace} {trigger_name}
```
