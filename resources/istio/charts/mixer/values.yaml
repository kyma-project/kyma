#
# mixer configuration
#
image: mixer

env:
  GODEBUG: gctrace=1
  # max procs should be ceil(cpu limit + 1)
  GOMAXPROCS: "6"

policy:
  # if policy is enabled, global.disablePolicyChecks has affect.
  enabled: false
  replicaCount: 1
  autoscaleEnabled: true
  autoscaleMin: 1
  autoscaleMax: 5
  cpu:
    targetAverageUtilization: 80
  rollingMaxSurge: 100%
  rollingMaxUnavailable: 25%

telemetry:
  enabled: true
  replicaCount: 1
  autoscaleEnabled: true
  autoscaleMin: 1
  autoscaleMax: 5
  cpu:
    targetAverageUtilization: 80
  rollingMaxSurge: 100%
  rollingMaxUnavailable: 25%
  sessionAffinityEnabled: false

  # mixer load shedding configuration.
  # When mixer detects that it is overloaded, it starts rejecting grpc requests.
  loadshedding:
    # disabled, logonly or enforce
    mode: enforce
    # based on measurements 100ms p50 translates to p99 of under 1s. This is ok for telemetry which is inherently async.
    latencyThreshold: 100ms
  resources:
    requests:
      cpu: 1000m
      memory: 1G
    limits:
      # It is best to do horizontal scaling of mixer using moderate cpu allocation.
      # We have experimentally found that these values work well.
      cpu: 4800m
      memory: 4G

  # Set reportBatchMaxEntries to 0 to use the default batching behavior (i.e., every 100 requests). 
  # A positive value indicates the number of requests that are batched before telemetry data 
  # is sent to the mixer server
  reportBatchMaxEntries: 100

  # Set reportBatchMaxTime to 0 to use the default batching behavior (i.e., every 1 second). 
  # A positive time value indicates the maximum wait time since the last request will telemetry data 
  # be batched before being sent to the mixer server
  reportBatchMaxTime: 1s

podAnnotations: {}
nodeSelector: {}
tolerations: []

# Specify the pod anti-affinity that allows you to constrain which nodes
# your pod is eligible to be scheduled based on labels on pods that are
# already running on the node rather than based on labels on nodes.
# There are currently two types of anti-affinity:
#    "requiredDuringSchedulingIgnoredDuringExecution"
#    "preferredDuringSchedulingIgnoredDuringExecution"
# which denote "hard" vs. "soft" requirements, you can define your values
# in "podAntiAffinityLabelSelector" and "podAntiAffinityTermLabelSelector"
# correspondingly.
# For example:
# podAntiAffinityLabelSelector:
# - key: security
#   operator: In
#   values: S1,S2
#   topologyKey: "kubernetes.io/hostname"
# This pod anti-affinity rule says that the pod requires not to be scheduled
# onto a node if that node is already running a pod with label having key
# "security" and value "S1".
podAntiAffinityLabelSelector: []
podAntiAffinityTermLabelSelector: []

adapters:
  kubernetesenv:
    enabled: true

  # stdio is a debug adapter in istio-telemetry, it is not recommended for production use.
  stdio:
    enabled: false
    outputAsJson: true
  prometheus:
    enabled: true
    metricsExpiryDuration: 10m
  # Setting this to false sets the useAdapterCRDs mixer startup argument to false
  useAdapterCRDs: false
