###############################
#                             #
#  NATS Server Configuration  #
#                             #
###############################
nats:
  pullPolicy: IfNotPresent
  ports:
    client: 4222
    monitoring: 8222
    cluster: 6222
    metrics: 7777
    leafnodes: 7422
    gateways: 7522

  # Toggle profiling.
  # This enables nats-server pprof (profiling) port, so you can see goroutines
  # stacks, memory heap sizes, etc.
  profiling:
    enabled: false
    port: 6000

  # Toggle using health check probes to better detect failures.
  healthcheck:
    # Enable /healthz startupProbe for controlled upgrades of NATS JetStream
    enableHealthz: true

    # Enable liveness checks.  If this fails, then the NATS Server will restarted.
    liveness:
      enabled: true

      initialDelaySeconds: 10
      timeoutSeconds: 5
      # NOTE: liveness check + terminationGracePeriodSeconds can introduce unecessarily long outages
      # due to the coupling between liveness probe and terminationGracePeriodSeconds.
      # To avoid this, we make the periodSeconds of the liveness check to be about half the default
      # time that it takes for lame duck graceful stop.
      #
      # In case of using Kubernetes +1.22 with probe-level terminationGracePeriodSeconds
      # we could revise this but for now keep a minimal liveness check.
      #
      # More info:
      #
      #  https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#probe-level-terminationgraceperiodseconds
      #  https://github.com/kubernetes/kubernetes/issues/64715
      #
      periodSeconds: 30
      successThreshold: 1
      failureThreshold: 3
      # Only for Kubernetes +1.22 that have pod level probes enabled.
      terminationGracePeriodSeconds:

    # Periodically check for the server to be ready for connections while
    # the NATS container is running.
    # Disabled by default since covered by startup probe and it is the same
    # as the liveness check.
    readiness:
      enabled: true

      initialDelaySeconds: 10
      timeoutSeconds: 5
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 3

    # Enable startup checks to confirm server is ready for traffic.
    # This is recommended for JetStream deployments since in cluster mode
    # it will try to ensure that the server is ready to serve streams.

    # since this check is recommended to use in cluster mode and the cluster mode ist disabled per default
    # this check is also defaulted to false
    startup:
      enabled: false

      initialDelaySeconds: 10
      timeoutSeconds: 5
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 30

  # Adds a hash of the ConfigMap as a pod annotation
  # This will cause the StatefulSet to roll when the ConfigMap is updated
  configChecksumAnnotation: true

  # securityContext for the nats container
  securityContext:
    allowPrivilegeEscalation: false
    privileged: false

  # Toggle to disable client advertisements (connect_urls),
  # in case of running behind a load balancer
  # it might be required to disable advertisements.
  advertise: true

  # The number of connect attempts against discovered routes.
  connectRetries: 120

  # selector matchLabels for the server and service.
  # If left empty defaults are used.
  # This is helpful if you are updating from Chart version <=7.4
  selectorLabels: {}

  resources:
    limits:
      cpu: 20m
      memory: 64Mi
    requests:
      cpu: 5m
      memory: 16Mi

  # Server settings.
  limits:
    maxConnections:
    maxSubscriptions:
    maxControlLine:
    maxPayload:

    writeDeadline:
    maxPending:
    maxPings:

    # How many seconds should pass before sending a PING
    # to a client that has no activity.
    pingInterval:

    # grace period after pod begins shutdown before starting to close client connections
    lameDuckGracePeriod: "10s"

    # duration over which to slowly close client connections after lameDuckGracePeriod has passed
    lameDuckDuration: "120s"

  # terminationGracePeriodSeconds determines how long to wait for graceful shutdown
  # this should be at least `lameDuckGracePeriod` + `lameDuckDuration` + 20s shutdown overhead
  terminationGracePeriodSeconds: 150

  logging:
    debug: true
    trace: true
    logtime:
    connectErrorReports:
    reconnectErrorReports:

  jetstream:
    #enabled: JetStream can be enabled via .Values.global.jetstream.enabled

    # Jetstream Domain
    domain:

    # Jetstream Unique Tag prevent placing a stream in the same availability zone twice.
    uniqueTag:

    ##########################
    #                        #
    #  Jetstream Encryption  #
    #                        #
    ##########################
    encryption:
      # Use key if you want to provide the key via Helm Values
      # key: random_key

      # Use a secret reference if you want to get a key from a secret
      # secret:
      #   name: "nats-jetstream-encryption"
      #   key: "key"

    #############################
    #                           #
    #  Jetstream Memory Storage #
    #                           #
    #############################
    memStorage:
      enabled: true
      size: 1Gi

    ############################
    #                          #
    #  Jetstream File Storage  #
    #                          #
    ############################
    fileStorage:
      #enabled: JetStream fileStorage can be enabled if .Values.global.jetstream.storage equals "file"
      storageDirectory: /data

      # Set for use with existing PVC
      # existingClaim: jetstream-pvc
      # claimStorageSize: 1Gi

      # Use below block to create new persistent volume
      # only used if existingClaim is not specified
      size: 1Gi
      # storageClassName: ""
      accessModes:
        - ReadWriteOnce
      annotations:
      # key: "value"

# Authentication setup
auth:
  enabled: false

  # basic:
  #   noAuthUser:
  #   # List of users that can connect with basic auth,
  #   # that belong to the global account.
  #   users:

  #   defaultPermissions:
  #     publish: ["SANDBOX.*"]
  #     subscribe: ["SANDBOX.>"]

  #   # List of accounts with users that can connect
  #   # using basic auth.
  #   accounts:

  # Reference to the Operator JWT.
  # operatorjwt:
  #   configMap:
  #     name: operator-jwt
  #     key: KO.jwt

  # Token authentication
  # token:

  # NKey authentication
  # nkeys:
  #   users:

  # Public key of the System Account
  # systemAccount:

  resolver:
    # Disables the resolver by default
    type: none

    ##########################################
    #                                        #
    # Embedded NATS Account Server Resolver  #
    #                                        #
    ##########################################
    # type: full

    # If the resolver type is 'full', delete when enabled will rename the jwt.
    allowDelete: false

    # Interval at which a nats-server with a nats based account resolver will compare
    # it's state with one random nats based account resolver in the cluster and if needed,
    # exchange jwt and converge on the same set of jwt.
    interval: 2m

    # Operator JWT
    operator:

    # System Account Public NKEY
    systemAccount:

    # resolverPreload:
    #   <ACCOUNT>: <JWT>

    # Directory in which the account JWTs will be stored.
    store:
      dir: "/accounts/jwt"

      # Size of the account JWT storage.
      size: 1Gi

      # StorageClass of JWT storage claim.
      # storageClassName: ""

    ##############################
    #                            #
    # Memory resolver settings   #
    #                            #
    ##############################
    # type: memory
    #
    # Use a configmap reference which will be mounted
    # into the container.
    #
    # configMap:
    #   name: nats-accounts
    #   key: resolver.conf

    ##########################
    #                        #
    #  URL resolver settings #
    #                        #
    ##########################
    # type: URL
    # url: "http://nats-account-server:9090/jwt/v1/accounts/"

nameOverride: ""

# Toggle whether to use setup a Pod Security Context
# ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
securityContext: {}
# securityContext:
#   fsGroup: 1000
#   runAsUser: 1000
#   runAsNonRoot: true

# Affinity for pod assignment
# ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              nats_cluster: eventing-nats
          topologyKey: kubernetes.io/hostname
        weight: 100

# Annotations to add to the NATS pods
# ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
podAnnotations: {
  sidecar.istio.io/inject: "false"
}

# Annotations to add to the NATS StatefulSet
statefulSetAnnotations: {}

# Labels to add to the pods of the NATS StatefulSet
statefulSetPodLabels:
  nats_cluster: eventing-nats

# Annotations to add to the NATS Service
serviceAnnotations: {}

cluster:
  enabled: false
  name: eventing-nats
  replicas: 3
  noAdvertise: false

appProtocol:
  enabled: true

# Cluster Domain configured on the kubelets
# https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
k8sClusterDomain: cluster.local

# Add labels to all the deployed resources
commonLabels: {}


# podManagementPolicy controls how pods are created during initial scale up,
# when replacing pods on nodes, or when scaling down.
podManagementPolicy: OrderedReady

# Prometheus NATS Exporter configuration.
exporter:
  enabled: true
  pullPolicy: IfNotPresent
  securityContext: {}
  resources: {}
  # Prometheus operator ServiceMonitor support. Exporter has to be enabled
  serviceMonitor:
    enabled: true
    labels: {}
    annotations: {}
    path: /metrics
    # interval:
    # scrapeTimeout:
