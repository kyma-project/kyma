###############################
#                             #
#  NATS Server Configuration  #
#                             #
###############################
nats:
  pullPolicy: IfNotPresent
  ports:
    client: 4222
    monitoring: 8222
    cluster: 6222
    metrics: 7777
    leafnodes: 7422
    gateways: 7522

  # Toggle profiling.
  # This enables nats-server pprof (profiling) port, so you can see goroutines
  # stacks, memory heap sizes, etc.
  profiling:
    enabled: false
    port: 6000

  # Toggle using health check probes to better detect failures.
  healthcheck:
    # Enable /healthz startupProbe for controlled upgrades of NATS JetStream
    enableHealthz: true

    # Enable liveness checks.  If this fails, then the NATS Server will restarted.
    liveness:
      enabled: true

      initialDelaySeconds: 10
      timeoutSeconds: 5
      # NOTE: liveness check + terminationGracePeriodSeconds can introduce unecessarily long outages
      # due to the coupling between liveness probe and terminationGracePeriodSeconds.
      # To avoid this, we make the periodSeconds of the liveness check to be about half the default
      # time that it takes for lame duck graceful stop.
      #
      # In case of using Kubernetes +1.22 with probe-level terminationGracePeriodSeconds
      # we could revise this but for now keep a minimal liveness check.
      #
      # More info:
      #
      #  https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#probe-level-terminationgraceperiodseconds
      #  https://github.com/kubernetes/kubernetes/issues/64715
      #
      periodSeconds: 30
      successThreshold: 1
      failureThreshold: 3
      # Only for Kubernetes +1.22 that have pod level probes enabled.
      terminationGracePeriodSeconds:

    # Periodically check for the server to be ready for connections while
    # the NATS container is running.
    # Disabled by default since covered by startup probe and it is the same
    # as the liveness check.
    readiness:
      enabled: true

      initialDelaySeconds: 10
      timeoutSeconds: 5
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 3

    # Enable startup checks to confirm server is ready for traffic.
    # This is recommended for JetStream deployments since in cluster mode
    # it will try to ensure that the server is ready to serve streams.

    # since this check is recommended to use in cluster mode and the cluster mode ist disabled per default
    # this check is also defaulted to false
    startup:
      enabled: false

      initialDelaySeconds: 10
      timeoutSeconds: 5
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 30

  # Adds a hash of the ConfigMap as a pod annotation
  # This will cause the StatefulSet to roll when the ConfigMap is updated
  configChecksumAnnotation: false

  # Toggle to disable client advertisements (connect_urls),
  # in case of running behind a load balancer
  # it might be required to disable advertisements.
  advertise: true

  # The number of connect attempts against discovered routes.
  connectRetries: 120

  # selector matchLabels for the server and service.
  # If left empty defaults are used.
  # This is helpful if you are updating from Chart version <=7.4
  selectorLabels: {}

  resources:
    limits:
      cpu: 20m
      memory: 64Mi
    requests:
      cpu: 5m
      memory: 16Mi

  # Server settings.
  limits:
    maxConnections:
    maxSubscriptions:
    maxControlLine:
    maxPayload:

    writeDeadline:
    maxPending:
    maxPings:

    # How many seconds should pass before sending a PING
    # to a client that has no activity.
    pingInterval:

    # grace period after pod begins shutdown before starting to close client connections
    lameDuckGracePeriod: "10s"

    # duration over which to slowly close client connections after lameDuckGracePeriod has passed
    lameDuckDuration: "120s"

  # terminationGracePeriodSeconds determines how long to wait for graceful shutdown
  # this should be at least `lameDuckGracePeriod` + `lameDuckDuration` + 20s shutdown overhead
  terminationGracePeriodSeconds: 150

  logging:
    debug: true
    trace: true
    logtime:
    connectErrorReports:
    reconnectErrorReports:

  jetstream:
    # Jetstream Domain
    domain:

    # Jetstream Unique Tag prevent placing a stream in the same availability zone twice.
    uniqueTag:

    ##########################
    #                        #
    #  Jetstream Encryption  #
    #                        #
    ##########################
    encryption:
      # Use key if you want to provide the key via Helm Values
      # key: random_key

      # Use a secret reference if you want to get a key from a secret
      # secret:
      #   name: "nats-jetstream-encryption"
      #   key: "key"

    #############################
    #                           #
    #  Jetstream Memory Storage #
    #                           #
    #############################
    memStorage:
      enabled: true
      size: 1Gi

    ############################
    #                          #
    #  Jetstream File Storage  #
    #                          #
    ############################
    fileStorage:
      #enabled: JetStream fileStorage can be enabled if .Values.global.jetstream.storage equals "file"
      storageDirectory: /data

      # Set for use with existing PVC
      # existingClaim: jetstream-pvc
      # claimStorageSize: 1Gi

      # Use below block to create new persistent volume
      # only used if existingClaim is not specified
      size: 1Gi
      # storageClassName: ""
      accessModes:
        - ReadWriteOnce
      annotations:
      # key: "value"

# Authentication setup
auth:
  enabled: true
  resolver:
    ##############################
    #                            #
    # Memory resolver settings   #
    #                            #
    ##############################
    type: memory


nameOverride: ""

# Affinity for pod assignment
# ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              nats_cluster: eventing-nats
          topologyKey: topology.kubernetes.io/zone
        weight: 70
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              nats_cluster: eventing-nats
          topologyKey: kubernetes.io/hostname
        weight: 35
# Annotations to add to the NATS pods
# ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
podAnnotations: {
  sidecar.istio.io/inject: "false"
}

# Annotations to add to the NATS StatefulSet
statefulSetAnnotations: {}

# Labels to add to the pods of the NATS StatefulSet
statefulSetPodLabels:
  nats_cluster: eventing-nats

# Annotations to add to the NATS Service
serviceAnnotations: {}

cluster:
  enabled: false
  name: eventing-nats
  replicas: 3
  noAdvertise: false

appProtocol:
  enabled: true

# Cluster Domain configured on the kubelets
# https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
k8sClusterDomain: cluster.local

# Add labels to all the deployed resources
commonLabels: {}


# podManagementPolicy controls how pods are created during initial scale up,
# when replacing pods on nodes, or when scaling down.
podManagementPolicy: OrderedReady

# Prometheus NATS Exporter configuration.
exporter:
  enabled: true
  pullPolicy: IfNotPresent
  resources: {}
  # Prometheus operator ServiceMonitor support. Exporter has to be enabled
  serviceMonitor:
    enabled: true
    labels: {}
    annotations: {}
    path: /metrics
    # interval:
    # scrapeTimeout:
