apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ printf "%s-%s" (include "kube-prometheus-stack.fullname" .) "kyma-general.rules" | trunc 63 | trimSuffix "-" }}
  labels:
    app: {{ template "kube-prometheus-stack.name" . }}
{{ include "kube-prometheus-stack.labels" . | indent 4 }}
spec:
  groups:
  - name: general.rules
    rules:
    - record: fd_utilization
      expr: process_open_fds / process_max_fds
    - alert: FdExhaustionClose
      expr: predict_linear(fd_utilization[1h], 3600 * 4) > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        description: '{{`{{ $labels.job }}`}}: {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} instance
          will exhaust in file/socket descriptors within the next 4 hours'
    - alert: FdExhaustionClose
      expr: predict_linear(fd_utilization[10m], 3600) > 1
      for: 10m
      labels:
        severity: critical
      annotations:
        description: '{{`{{ $labels.job }}`}}: {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} instance
          will exhaust in file/socket descriptors within the next hour'
    - alert: APIServerLatencyWarning
      expr: apiserver_latency_seconds:quantile{quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        description: the API server has a 99th percentile latency of {{`{{ $value }}`}} seconds
          for {{`{{$labels.verb}}`}} {{`{{$labels.resource}}`}}
  - name: k8s.rules
    rules:
    - record: apiserver_latency_seconds:quantile
      expr: histogram_quantile(0.99, rate(apiserver_request_latencies_bucket[5m])) /
        1e+06
      labels:
        quantile: "0.99"
    - record: apiserver_latency:quantile_seconds
      expr: histogram_quantile(0.9, rate(apiserver_request_latencies_bucket[5m])) /
        1e+06
      labels:
        quantile: "0.9"
    - record: apiserver_latency_seconds:quantile
      expr: histogram_quantile(0.5, rate(apiserver_request_latencies_bucket[5m])) /
        1e+06
      labels:
        quantile: "0.5"
    - alert: APIServerLatencyWarningË‡
      expr: apiserver_latency_seconds:quantile{quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
      for: 10m
      labels:
        severity: warning
      annotations:
        description: the API server has a 99th percentile latency of {{`{{ $value }}`}} seconds
          for {{`{{$labels.verb}}`}} {{`{{$labels.resource}}`}}
    - alert: APIServerLatencyHighCritical
      expr: apiserver_latency_seconds:quantile{quantile="0.99",subresource!="log",verb!~"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$"} > 10
      for: 10m
      labels:
        severity: critical
      annotations:
        description: the API server has a 99th percentile latency of {{`{{ $value }}`}} seconds
          for {{`{{$labels.verb}}`}} {{`{{$labels.resource}}`}}
    - alert: APIServerErrorsHighWarning
      expr: rate(apiserver_request_total{code=~"^(?:5..)$"}[5m]) / rate(apiserver_request_total[5m])
        * 100 > 2
      for: 10m
      labels:
        severity: warning
      annotations:
        description: API server returns errors for {{`{{ $value }}`}}% of requests for job `{{`{{$labels.job}}`}}`/HTTP `{{`{{$labels.verb}}`}}`
    - alert: APIServerErrorsHighCritical
      expr: rate(apiserver_request_total{code=~"^(?:5..)$"}[5m]) / rate(apiserver_request_total[5m])
        * 100 > 5
      for: 10m
      labels:
        severity: critical
      annotations:
        description: API server returns errors for {{`{{ $value }}`}}% of requests for job `{{`{{$labels.job}}`}}`/HTTP `{{`{{$labels.verb}}`}}`
    - alert: K8SApiserverDown
      expr: absent(up{job="apiserver"} == 1)
      for: 20m
      labels:
        severity: critical
      annotations:
        description: No API servers are reachable or all have disappeared from service
          discovery
    - alert: K8sCertificateExpirationNotice2
      labels:
        severity: warning
      annotations:
        description: Kubernetes API Certificate is expiring soon (less than 7 days)
      expr: sum(apiserver_client_certificate_expiration_seconds_bucket{le="604800"}) > 0
    - alert: K8sCertificateExpirationNotice
      labels:
        severity: critical
      annotations:
        description: Kubernetes API Certificate is expiring in less than 1 day
      expr: sum(apiserver_client_certificate_expiration_seconds_bucket{le="86400"}) > 0
    - alert: KubePodOOMKilled
      expr: sum_over_time(kube_pod_container_status_terminated_reason{reason="OOMKilled"}[5m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        description: Pod {{`{{`}} $labels.namespace {{`}}`}}/{{`{{`}} $labels.pod {{`}}`}} ({{`{{`}} $labels.container {{`}}`}}) is OOMKilled for 5 minutes.
    - alert: PVC90PercentFull
      expr: kubelet_volume_stats_used_bytes{namespace="kube-system",exported_namespace=~"kyma-.*|kube-.*|istio-.*|natss" } / kubelet_volume_stats_capacity_bytes{namespace="kube-system",exported_namespace=~"kyma-.*|kube-.*|istio-.*|natss"} * 100 > 90
      for: 10m
      labels:
        severity: critical
      annotations:
        description: "PVC {{`{{$labels.exported_namespace}}`}}/{{`{{$labels.persistentvolumeclaim}}`}} is using {{`{{$value}}`}} % of the available volume"
    - alert: APIServerEventsRejected
      expr: sum(rate(apiserver_audit_requests_rejected_total{job=~"apiserver"}[1m])) > 0
      for: 1m
      labels:
        severity: critical
      annotations:
        description: "API Server events are being rejected. Currently rejected events: {{`{{$value}}`}}"
  

