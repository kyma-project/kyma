# Default values for fluent-bit.

# kind -- DaemonSet or Deployment
kind: DaemonSet

# replicaCount -- Only applicable if kind=Deployment
replicaCount: 1

image:
  repository: eu.gcr.io/kyma-project/incubator/develop/fluent-bit
  pullPolicy: IfNotPresent
  tag: 1.5.7-2ac351ac

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name:

rbac:
  create: true

podSecurityPolicy:
  create: false

podSecurityContext:
  {}
  # fsGroup: 2000
dnsConfig: {}
  # nameservers:
  #   - 1.2.3.4
  # searches:
  #   - ns1.svc.cluster-domain.example
  #   - my.dns.search.suffix
  # options:
  #   - name: ndots
#     value: "2"
#   - name: edns0
securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 2020
  labels:
    {}
  annotations:
    {}
    # prometheus.io/path: "/api/v1/metrics/prometheus"
    # prometheus.io/port: "2020"
    # prometheus.io/scrape: "true"

serviceMonitor:
  enabled: true
  # namespace: monitoring
  # interval: 10s
  # scrapeTimeout: 10s
  # selector:
  #  prometheus: my-prometheus

livenessProbe:
  httpGet:
    path: /
    port: http

readinessProbe:
  httpGet:
    path: /
    port: http

resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 20m
    memory: 50Mi

nodeSelector: {}

tolerations: []

affinity: {}

podAnnotations: {}

podLabels: {}

priorityClassName: ""

env: []

envFrom: []

extraPorts: []
#   - port: 5170
#     containerPort: 5170
#     protocol: TCP
#     name: tcp

extraVolumes: []

extraVolumeMounts: []

updateStrategy: {}
  # type: RollingUpdate
  # rollingUpdate:
  #   maxUnavailable: 1

# Make use of a pre-defined configmap instead of the one templated here
existingConfigMap: ""

# Fluentbit configuration section
# https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/configuration-file
config:
  service:
    flush: 1
    logLevel: error
    daemon: "off"
    http:
      server: "On"
      listen: 0.0.0.0
      port: 2020
  inputs:
    tail:
      tag: kube.*
      path: /var/log/containers/*.log
      excludePath:
      parser: docker
      # If enabled, the plugin will recombine split Docker log lines before passing them to any parser as configured above. This mode cannot be used at the same time as Multiline.
      dockerMode: On
      # Wait period time in seconds to flush queued unfinished split lines.
      dockerModeFlush: 4
      # Specify the database file to keep track of monitored files and offsets.
      db: /var/log/flb_kube.db
      # Set a limit of memory that Tail plugin can use when appending data to the Engine. If the limit is reach, it will be paused; when the data is flushed it resumes.
      memBufLimit: 5MB
      # When a monitored file reach it buffer capacity due to a very long line (Buffer_Max_Size), the default behavior is to stop monitoring that file. Skip_Long_Lines alter that behavior and instruct Fluent Bit to skip long lines and continue processing other lines that fits into the buffer size. Default: Off
      skipLongLines: On
      # The interval of refreshing the list of watched files. Default is 60 seconds.
      refreshInterval: 10
      #	Set the initial buffer size to read files data. This value is used too to increase buffer size. The value must be according to the Unit Size specification. Default: 32k
      bufferChunkSize:
      # Set the limit of the buffer size per monitored file. When a buffer needs to be increased (e.g: very long lines), this value is used to restrict how much the memory buffer can grow. If reading a file exceed this limit, the file is removed from the monitored file list. The value must be according to the Unit Size specification. Default: Buffer_Chunk_Size
      bufferMaxSize:
    tailLoki:
      path: /var/log/containers/*.log
      excludePath: 
      parser: docker
      # If enabled, the plugin will recombine split Docker log lines before passing them to any parser as configured above. This mode cannot be used at the same time as Multiline.
      dockerMode: On
      # Wait period time in seconds to flush queued unfinished split lines.
      dockerModeFlush: 4
      # Specify the database file to keep track of monitored files and offsets.
      db: /var/log/flb_kube_loki.db
      # Set a limit of memory that Tail plugin can use when appending data to the Engine. If the limit is reach, it will be paused; when the data is flushed it resumes.
      memBufLimit: 5MB
      # When a monitored file reach it buffer capacity due to a very long line (Buffer_Max_Size), the default behavior is to stop monitoring that file. Skip_Long_Lines alter that behavior and instruct Fluent Bit to skip long lines and continue processing other lines that fits into the buffer size. Default: Off
      skipLongLines: On
      # The interval of refreshing the list of watched files. Default is 60 seconds.
      refreshInterval: 10
      #	Set the initial buffer size to read files data. This value is used too to increase buffer size. The value must be according to the Unit Size specification. Default: 32k
      bufferChunkSize:
      # Set the limit of the buffer size per monitored file. When a buffer needs to be increased (e.g: very long lines), this value is used to restrict how much the memory buffer can grow. If reading a file exceed this limit, the file is removed from the monitored file list. The value must be according to the Unit Size specification. Default: Buffer_Chunk_Size
      bufferMaxSize:
      exclude:
        namespaces:
    additional: ""
  filters:
    kubernetes:
      # When enabled, it checks if the log field content is a JSON string map, if so, it append the map fields as part of the log structure.
      mergeLog: "On"
      keepLog: "On"
      match: "*"
      # When Merge_Log is enabled, the filter tries to assume the log field from the incoming message is a JSON string message and make a structured representation of it at the same level of the log field in the map. Now if Merge_Log_Key is set (a string name), all the new structured fields taken from the original log content are inserted under the new key.
      mergeLogKey: ""
      parser: On
      exclude: On
    additional: ""
  outputs:
    loki:
      enabled: true
      serviceName: "logging-loki"
      servicePort: 3100
      serviceScheme: http
      servicePath: /loki/api/v1/push
      config:
        port: 2020
        loglevel: warn
        lineFormat: json
        removeKeys:
          - kubernetes
          - stream
        labels: '{job="fluent-bit"}'
        labelMap:
          kubernetes:
            namespace_name: namespace
            labels:
              app: app
              release: release
              "serverless.kyma-project.io/function-name": function
              "serverless.kyma-project.io/uuid": functionUID
            host: node
            container_name: container
            pod_name: pod
          stream: stream
    es:
      enabled: false
      match: "*"
      host: elasticsearch
      port: 443
      logstashFormat: On
      logstashPrefix: flb
      index: flb_logs_write
      retryLimit: False
      generateID: On
      replaceDots: On
      bufferSize: False
      # default timeKey: @timestamp
      timeKey:
      # Optional credentials for Elastic X-Pack access
      httpUser:
      httpPasswd:
      type: flb_type
      # Optional TLS encryption to ElasticSeaarch instance
      tls:
        enabled: true
        verify: On
        # TLS certificate for the Elastic (in base64 PEM format). Use if tls=on and tls_verify=on.
        ca: ""
        cert: ""
        key: ""
        # optional password for tls.key_file file
        keyPasswd: ""
        # TLS debugging levels = 1-5
        debug: 1
    forward:
      enabled: false
      match: "*"
      host: log-forwarder
      port: 24224
      tls:
        enabled: false
        verify: Off
        # TLS certificate for the Elastic (in base64 PEM format). Use if tls=on and tls_verify=on.
        ca: ""
        cert: ""
        key: ""
        # TLS debugging levels = 1-5
        debug: 1
    http:
      enabled: false
      match: "*"
      host: "127.0.0.1"
      port: 80
      uri: "/"
      httpUser: ""
      httpPasswd: ""
      tls:
        enabled: false
        verify: on
        ca: ""
        cert: ""
        key: ""
        debug: 1
      ## Specify the data format to be used in the HTTP request body
      ## Can be either 'msgpack' or 'json'
      format: msgpack
      compress: ""
      jsonDateKey: ""
      jsonDateFormat: ""
      headers: []
    additional: ""
  
  # extra can be used to pass extra configuration to Fluent Bit. Find below a sample configuration.
  extra: #|
  #  [FILTER]
  #      Name              test
  #      Match             *
  #  [OUTPUT]
  #      Name              test
  #      Match             *
  ## https://docs.fluentbit.io/manual/pipeline/parsers
  parsers:
    additional: ""
  script: #|
  # myScript() {}

# Defines an entry to add an external service to the service mesh
# By default, the fluent-bit daemon will be part of the service-mesh (having istio sidecar injection enabled). In some cases the sidecar doesn't allow access to external services through https using none standard ports (not 443 or 8443)
# A typical error message will look like 'routines:ssl3_get_record:wrong version number'. In such cases, add an entry for the external service like below
#externalServiceEntry:
#  hosts:
#  - my.example.host.dns
#  ports:
#    - number: 8081
#      name: https
#      protocol: TLS
#  resolution: DNS
