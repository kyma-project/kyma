---
# Source: monitoring/charts/grafana/templates/podsecuritypolicy.yaml

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    # Default set from Docker, without DAC_OVERRIDE or CHOWN
    - FOWNER
    - FSETID
    - KILL
    - SETGID
    - SETUID
    - SETPCAP
    - NET_BIND_SERVICE
    - NET_RAW
    - SYS_CHROOT
    - MKNOD
    - AUDIT_WRITE
    - SETFCAP
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: false

---
# Source: monitoring/charts/kube-state-metrics/templates/podsecuritypolicy.yaml

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-kube-state-metrics
  labels:
    helm.sh/chart: kube-state-metrics-2.8.2
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
spec:
  privileged: false
  volumes:
    - 'secret'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false

---
# Source: monitoring/charts/prometheus-node-exporter/templates/psp.yaml

apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-prometheus-node-exporter
  namespace: default
  labels:     
    app: prometheus-node-exporter
    helm.sh/chart: prometheus-node-exporter-1.9.1
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
    release: release-name
    chart: prometheus-node-exporter-1.9.1
    jobLabel: node-exporter
    
spec:
  privileged: false
  # Required to prevent escalations to root.
  # allowPrivilegeEscalation: false
  # This is redundant with non-root + disallow privilege escalation,
  # but we can provide it for defense in depth.
  #requiredDropCapabilities:
  #  - ALL
  # Allow core volume types.
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
    - 'hostPath'
  hostNetwork: true
  hostIPC: false
  hostPID: true
  hostPorts:
    - min: 0
      max: 65535
  runAsUser:
    # Permits the container to run with root privileges as well.
    rule: 'RunAsAny'
  seLinux:
    # This policy assumes the nodes are using AppArmor rather than SELinux.
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 0
        max: 65535
  readOnlyRootFilesystem: false

---
# Source: monitoring/charts/grafana/templates/secret.yaml

apiVersion: v1
kind: Secret
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
type: Opaque
data:
  admin-user: "YWRtaW4="
  admin-password: "cHJvbS1vcGVyYXRvcg=="
  ldap-toml: ""

---
# Source: monitoring/charts/grafana/templates/configmap-dashboard-provider.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
  name: release-name-grafana-config-dashboards
  namespace: default
data:
  provider.yaml: |-
    apiVersion: 1
    providers:
    - name: 'sidecarProvider'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      allowUiUpdates: false
      options:
        path: /tmp/dashboards

---
# Source: monitoring/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
data:
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/data
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning

---
# Source: monitoring/templates/kyma-additions/aks-kubelet-monitoring-patch-pre-upgrade-cleanup.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: aks-kubelet-monitoring-cleanup-patch
  namespace: kyma-system
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
data:
  akspatchcleanup.sh: |
    #!/usr/bin/env sh
    set -e

    if [[ "$(kubectl get nodes -l kubernetes.azure.com/role=agent)" ]]; then
      echo "AKS Cluster found, applying patch..."
      kubectl delete servicemonitors.monitoring.coreos.com monitoring-kubelet -n kyma-system --ignore-not-found
    else
        echo "No AKS Cluster found, skipping..."
    fi
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aks-kubelet-monitoring-cleanup-patch
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aks-kubelet-monitoring-cleanup-patch
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
rules:
  - apiGroups: [""]
    resources: ["serviceaccounts", "services", "pods", "nodes"]
    verbs: ["get", "delete", "create", "watch", "list"]
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: ["get", "delete", "create", "watch", "list"]
  - apiGroups: ["extensions","apps"]
    resources: ["deployments", "statefulsets"]
    verbs: ["get", "delete", "create", "watch", "list"]
  - apiGroups: ["monitoring.coreos.com"]
    resources: ["alertmanagers", "prometheuses", "prometheuses/finalizers", "alertmanagers/finalizers", "servicemonitors", "podmonitors", "prometheusrules", "podmonitors"]
    verbs: ["get", "delete", "create", "watch", "list"]
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["get", "delete", "create", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: aks-kubelet-monitoring-cleanup-patch
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: aks-kubelet-monitoring-cleanup-patch
subjects:
  - kind: ServiceAccount
    name: aks-kubelet-monitoring-cleanup-patch
    namespace: kyma-system
---
apiVersion: batch/v1
kind: Job
metadata:
  name: aks-kubelet-monitoring-cleanup-patch
  namespace: kyma-system
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    sidecar.istio.io/inject: "false"
    helm.sh/hook-weight: "9"
spec:
  backoffLimit: 1
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
      name: aks-kubelet-monitoring-cleanup-patch
      labels:
        app: aks-kubelet-monitoring-cleanup-patch
    spec:
      serviceAccountName: aks-kubelet-monitoring-cleanup-patch
      restartPolicy: Never
      containers:
        - name: aks-kubelet-monitoring-cleanup-patch
          image: eu.gcr.io/kyma-project/test-infra/alpine-kubectl:v20200617-32c1f3ff
          command: ["/scripts/akspatchcleanup.sh"]
          volumeMounts:
            - name: aks-kubelet-monitoring-cleanup-patch
              mountPath: /scripts
          terminationMessagePolicy: "FallbackToLogsOnError"
          resources:
            requests:
              cpu: 200m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 128Mi
      volumes:
        - name: aks-kubelet-monitoring-cleanup-patch
          configMap:
            name: aks-kubelet-monitoring-cleanup-patch
            defaultMode: 0744


---
# Source: monitoring/templates/kyma-additions/delete-dns-patch.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pre-delete-delete-monitoring-dns-patch
  namespace: kyma-system
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
data:
  dns-del-patch.sh: |
    #!/usr/bin/env sh
    set -e
    kubectl delete --ignore-not-found clusterrolebindings.rbac.authorization.k8s.io -n kyma-system monitoring-dns-patch
    kubectl delete --ignore-not-found clusterroles.rbac.authorization.k8s.io -n kyma-system monitoring-dns-patch
    kubectl delete --ignore-not-found serviceaccounts -n kyma-system monitoring-dns-patch
    kubectl delete --ignore-not-found configmaps -n kyma-system monitoring-dns-patch
    kubectl delete --ignore-not-found jobs.batch -n kyma-system monitoring-dns-patch

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pre-delete-delete-monitoring-dns-patch
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
    
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pre-delete-delete-monitoring-dns-patch
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
rules:
  - apiGroups: [""]
    resources: ["serviceaccounts", "configmaps"]
    verbs: ["delete"]
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["delete"]
  - apiGroups: ["rbac.authorization.k8s.io"]
    resources: ["clusterroles", "clusterrolebindings"]
    verbs: ["delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pre-delete-delete-monitoring-dns-patch
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: pre-delete-delete-monitoring-dns-patch
subjects:
  - kind: ServiceAccount
    name: pre-delete-delete-monitoring-dns-patch
    namespace: kyma-system
---
apiVersion: batch/v1
kind: Job
metadata:
  name: pre-delete-delete-monitoring-dns-patch
  namespace: kyma-system
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: hook-succeeded
    sidecar.istio.io/inject: "false"
    helm.sh/hook-weight: "9"
spec:
  backoffLimit: 1
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
      name: pre-delete-delete-monitoring-dns-patch
      labels:
        app: pre-delete-delete-monitoring-dns-patch
    spec:
      serviceAccountName: pre-delete-delete-monitoring-dns-patch
      restartPolicy: Never
      containers:
        - name: pre-delete-delete-monitoring-dns-patch
          image: eu.gcr.io/kyma-project/test-infra/alpine-kubectl:v20200617-32c1f3ff
          command: ["/scripts/dns-del-patch.sh"]
          volumeMounts:
            - name: pre-delete-delete-monitoring-dns-patch
              mountPath: /scripts
          terminationMessagePolicy: "FallbackToLogsOnError"
          resources:
            requests:
              cpu: 200m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 128Mi
      volumes:
        - name: pre-delete-delete-monitoring-dns-patch
          configMap:
            name: pre-delete-delete-monitoring-dns-patch
            defaultMode: 0744

---
# Source: monitoring/templates/kyma-additions/monitoring-dns-patch.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-dns-patch
  namespace: kyma-system
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
data:
  dnspatch.sh: |
    #!/bin/bash
    set -e

    deployments=$(kubectl get deployment -n kube-system -o name)

    if [[ "$deployments" =~ "coredns" ]]; then
      echo "CoreDNS deployment found, deleting KubeDNS resources..."
      kubectl delete service monitoring-kube-dns -n kube-system --ignore-not-found
      kubectl delete servicemonitor monitoring-kube-dns -n kyma-system --ignore-not-found
    else
      echo "CoreDNS deployment not found, deleting CoreDNS resources..."
      kubectl delete service monitoring-coredns -n kube-system --ignore-not-found
      kubectl delete servicemonitor monitoring-coredns -n kyma-system --ignore-not-found
    fi
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: monitoring-dns-patch
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-dns-patch
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
rules:
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get", "delete"]
  - apiGroups: ["extensions","apps"]
    resources: ["deployments"]
    verbs: ["get", "list"]
  - apiGroups: ["monitoring.coreos.com"]
    resources: ["servicemonitors"]
    verbs: ["get", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: monitoring-dns-patch
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: monitoring-dns-patch
subjects:
  - kind: ServiceAccount
    name: monitoring-dns-patch
    namespace: kyma-system
---
apiVersion: batch/v1
kind: Job
metadata:
  name: monitoring-dns-patch
  namespace: kyma-system
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded
    sidecar.istio.io/inject: "false"
    helm.sh/hook-weight: "9"
spec:
  backoffLimit: 1
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
      name: monitoring-dns-patch
      labels:
        app: monitoring-dns-patch
    spec:
      serviceAccountName: monitoring-dns-patch
      restartPolicy: Never
      containers:
        - name: monitoring-dns-patch
          image: eu.gcr.io/kyma-project/test-infra/alpine-kubectl:v20200617-32c1f3ff
          command: ["/scripts/dnspatch.sh"]
          volumeMounts:
            - name: monitoring-dns-patch
              mountPath: /scripts
          terminationMessagePolicy: "FallbackToLogsOnError"
          resources:
            requests:
              cpu: 200m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 128Mi
      volumes:
        - name: monitoring-dns-patch
          configMap:
            name: monitoring-dns-patch
            defaultMode: 0744

---
# Source: monitoring/charts/grafana/templates/pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
  finalizers:
    - kubernetes.io/pvc-protection
    
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "1Gi"
---
# Source: monitoring/charts/grafana/templates/serviceaccount.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
  name: release-name-grafana
  namespace: default

---
# Source: monitoring/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: kube-state-metrics-2.8.2
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
  name: release-name-kube-state-metrics
  namespace: default
imagePullSecrets:
  []
  
---
# Source: monitoring/charts/prometheus-node-exporter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-prometheus-node-exporter
  namespace: default
  labels:
    app: prometheus-node-exporter
        
    app: prometheus-node-exporter
    helm.sh/chart: prometheus-node-exporter-1.9.1
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
    release: release-name
    chart: prometheus-node-exporter-1.9.1
    jobLabel: node-exporter
    
imagePullSecrets:
  []
  
---
# Source: monitoring/charts/grafana/templates/clusterrole.yaml

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
  name: release-name-grafana-clusterrole
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["configmaps", "secrets"]
  verbs: ["get", "watch", "list"]

---
# Source: monitoring/charts/kube-state-metrics/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    helm.sh/chart: kube-state-metrics-2.8.2
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
  name: release-name-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling.k8s.io"]
  resources:
    - verticalpodautoscalers
  verbs: ["list", "watch"]

---
# Source: monitoring/charts/prometheus-node-exporter/templates/psp-clusterrole.yaml

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: psp-release-name-prometheus-node-exporter
  labels:     
    app: prometheus-node-exporter
    helm.sh/chart: prometheus-node-exporter-1.9.1
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
    release: release-name
    chart: prometheus-node-exporter-1.9.1
    jobLabel: node-exporter
    
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - release-name-prometheus-node-exporter

---
# Source: monitoring/charts/grafana/templates/clusterrolebinding.yaml

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-grafana-clusterrolebinding
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
subjects:
  - kind: ServiceAccount
    name: release-name-grafana
    namespace: default
roleRef:
  kind: ClusterRole
  name: release-name-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: monitoring/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  labels:
    helm.sh/chart: kube-state-metrics-2.8.2
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
  name: release-name-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: release-name-kube-state-metrics
  namespace: default
---
# Source: monitoring/charts/prometheus-node-exporter/templates/psp-clusterrolebinding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: psp-release-name-prometheus-node-exporter
  labels:     
    app: prometheus-node-exporter
    helm.sh/chart: prometheus-node-exporter-1.9.1
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
    release: release-name
    chart: prometheus-node-exporter-1.9.1
    jobLabel: node-exporter
    
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp-release-name-prometheus-node-exporter
subjects:
  - kind: ServiceAccount
    name: release-name-prometheus-node-exporter
    namespace: default

---
# Source: monitoring/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
rules:
- apiGroups:      ['extensions']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [release-name-grafana]

---
# Source: monitoring/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-grafana
subjects:
- kind: ServiceAccount
  name: release-name-grafana
  namespace: default
---
# Source: monitoring/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
spec:
  type: ClusterIP
  ports:
    - name: http-service
      port: 80
      protocol: TCP
      targetPort: 3000

  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name

---
# Source: monitoring/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-state-metrics
  namespace: default
  labels:
    helm.sh/chart: kube-state-metrics-2.8.2
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  selector:
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name

---
# Source: monitoring/charts/prometheus-node-exporter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-prometheus-node-exporter
  namespace: default
  annotations:
    prometheus.io/scrape: "true"
    
  labels:     
    app: prometheus-node-exporter
    helm.sh/chart: prometheus-node-exporter-1.9.1
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
    release: release-name
    chart: prometheus-node-exporter-1.9.1
    jobLabel: node-exporter
    
spec:
  type: ClusterIP
  ports:
    - port: 9100
      targetPort: 9100
      protocol: TCP
      name: metrics
  selector:
    app: prometheus-node-exporter
    release: release-name

---
# Source: monitoring/charts/prometheus-node-exporter/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-prometheus-node-exporter
  namespace: default
  labels:     
    app: prometheus-node-exporter
    helm.sh/chart: prometheus-node-exporter-1.9.1
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
    release: release-name
    chart: prometheus-node-exporter-1.9.1
    jobLabel: node-exporter
    
spec:
  selector:
    matchLabels:
      app: prometheus-node-exporter
      release: release-name
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
    
  template:
    metadata:
      labels:         
        app: prometheus-node-exporter
        helm.sh/chart: prometheus-node-exporter-1.9.1
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Tiller
        release: release-name
        chart: prometheus-node-exporter-1.9.1
        jobLabel: node-exporter
        
    spec:
      serviceAccountName: release-name-prometheus-node-exporter
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        
      containers:
        - name: node-exporter
          image: "eu.gcr.io/kyma-project/incubator/develop/node-exporter:0.18.1-6f374cc6"
          imagePullPolicy: IfNotPresent
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --web.listen-address=$(HOST_IP):9100
            - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)
            - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
            
          env:
          - name: HOST_IP
            value: 0.0.0.0
          ports:
            - name: metrics
              containerPort: 9100
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: 9100
          readinessProbe:
            httpGet:
              path: /
              port: 9100
          resources:
            {}
            
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
      hostNetwork: true
      hostPID: true
      tolerations:
        - effect: NoSchedule
          operator: Exists
        
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys

---
# Source: monitoring/charts/grafana/templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-grafana
  namespace: default
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: release-name
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: release-name
        app: grafana
        
      annotations:
        checksum/config: 27fc5cb6cf930d23f17db135aa2e0df9c9bbe3a6d511137d460ce8029b573e60
        checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/sc-dashboard-provider-config: 12328e5e6da2a1d1e29ca7da80443c0c447e8730077b41d2ac5ee941f77edfaa
        checksum/secret: 484c45e1319420780f036ada07474f39b936ec434241f442e6f33bc60879b796
    spec:
      
      serviceAccountName: release-name-grafana
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsUser: 472
        
      initContainers:
        - name: init-chown-data
          image: "busybox:1.31.1"
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
          command: ["chown", "-R", "472:472", "/var/lib/grafana"]
          resources:
            {}
            
          volumeMounts:
            - name: storage
              mountPath: "/var/lib/grafana"
        - name: grafana-sc-datasources
          image: "eu.gcr.io/kyma-project/incubator/develop/k8s-sidecar:0.1.157-9831a065"
          imagePullPolicy: IfNotPresent
          env:
            - name: METHOD
              value: LIST
            - name: LABEL
              value: "grafana_datasource"
            - name: FOLDER
              value: "/etc/grafana/provisioning/datasources"
            - name: RESOURCE
              value: "both"
          resources:
            {}
            
          volumeMounts:
            - name: sc-datasources-volume
              mountPath: "/etc/grafana/provisioning/datasources"
      containers:
        - name: grafana-sc-dashboard
          image: "eu.gcr.io/kyma-project/incubator/develop/k8s-sidecar:0.1.157-9831a065"
          imagePullPolicy: IfNotPresent
          env:
            - name: METHOD
              value: 
            - name: LABEL
              value: "grafana_dashboard"
            - name: FOLDER
              value: "/tmp/dashboards"
            - name: RESOURCE
              value: "both"
          resources:
            {}
            
          volumeMounts:
            - name: sc-dashboard-volume
              mountPath: "/tmp/dashboards"
        - name: grafana
          image: "grafana/grafana:7.0.6"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: sc-dashboard-volume
              mountPath: "/tmp/dashboards"
      
            - name: sc-dashboard-provider
              mountPath: "/etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml"
              subPath: provider.yaml
            - name: sc-datasources-volume
              mountPath: "/etc/grafana/provisioning/datasources"
          ports:
            - name: http-service
              containerPort: 80
              protocol: TCP
            - name: grafana
              containerPort: 3000
              protocol: TCP
          env:
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-password
            - name: GF_AUTH_GENERIC_OAUTH_AUTH_URL
              value: "https://dex.dummy/auth"
            - name: GF_SERVER_ROOT_URL
              value: "https://grafana.dummy/"
            - name: "GF_AUTH_ANONYMOUS_ENABLED"
              value: "false"
            - name: "GF_AUTH_DISABLE_LOGIN_FORM"
              value: "true"
            - name: "GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP"
              value: "true"
            - name: "GF_AUTH_GENERIC_OAUTH_CLIENT_ID"
              value: "grafana"
            - name: "GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET"
              value: "apie4eeX6hiC9ainieli"
            - name: "GF_AUTH_GENERIC_OAUTH_ENABLED"
              value: "true"
            - name: "GF_AUTH_GENERIC_OAUTH_SCOPES"
              value: "openid email"
            - name: "GF_AUTH_GENERIC_OAUTH_TOKEN_URL"
              value: "http://dex-service:5556/token"
            - name: "GF_AUTH_OAUTH_AUTO_LOGIN"
              value: "true"
            - name: "GF_LOG_LEVEL"
              value: "warn"
            - name: "GF_LOG_MODE"
              value: "console"
            - name: "GF_PATHS_PROVISIONING"
              value: "/etc/grafana/provisioning"
            - name: "GF_USERS_AUTO_ASSIGN_ORG"
              value: "true"
            - name: "GF_USERS_AUTO_ASSIGN_ORG_ROLE"
              value: "Editor"
            - name: "GF_USERS_DEFAULT_THEME"
              value: "dark"
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
            
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
            
          resources:
            limits:
              memory: 200Mi
            requests:
              memory: 200Mi
            
      volumes:
        - name: config
          configMap:
            name: release-name-grafana
        - name: storage
          persistentVolumeClaim:
            claimName: release-name-grafana
        - name: sc-dashboard-volume
          emptyDir: {}
        - name: sc-dashboard-provider
          configMap:
            name: release-name-grafana-config-dashboards
        - name: sc-datasources-volume
          emptyDir: {}

---
# Source: monitoring/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kube-state-metrics
  namespace: default
  labels:
    helm.sh/chart: kube-state-metrics-2.8.2
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Tiller
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: kube-state-metrics
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: "release-name"
      annotations:
        sidecar.istio.io/inject: "false"
        
    spec:
      hostNetwork: false
      serviceAccountName: release-name-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsUser: 65534
      containers:
      - name: kube-state-metrics
        args:

        - --collectors=certificatesigningrequests


        - --collectors=configmaps


        - --collectors=cronjobs


        - --collectors=daemonsets


        - --collectors=deployments


        - --collectors=endpoints


        - --collectors=horizontalpodautoscalers


        - --collectors=ingresses


        - --collectors=jobs


        - --collectors=limitranges


        - --collectors=mutatingwebhookconfigurations


        - --collectors=namespaces


        - --collectors=networkpolicies


        - --collectors=nodes


        - --collectors=persistentvolumeclaims


        - --collectors=persistentvolumes


        - --collectors=poddisruptionbudgets


        - --collectors=pods


        - --collectors=replicasets


        - --collectors=replicationcontrollers


        - --collectors=resourcequotas


        - --collectors=secrets


        - --collectors=services


        - --collectors=statefulsets


        - --collectors=storageclasses


        - --collectors=validatingwebhookconfigurations


        - --collectors=verticalpodautoscalers


        - --collectors=volumeattachments



        imagePullPolicy: IfNotPresent
        image: "eu.gcr.io/kyma-project/external/quay.io/coreos/kube-state-metrics:v1.9.5"
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5

---
# Source: monitoring/charts/grafana/templates/kyma-additions/backendmodule.yaml

apiVersion: ui.kyma-project.io/v1alpha1
kind: BackendModule
metadata:
  name: grafana

---
# Source: monitoring/charts/grafana/templates/kyma-additions/stats-metrics-microfrontend.yaml

apiVersion: "ui.kyma-project.io/v1alpha1"
kind: ClusterMicroFrontend
metadata:
  name: statsmetricsmicrofrontend
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
    app:  statsmetricsmicrofrontend
spec:
  displayName: Metrics
  version: v1
  category: Diagnostics
  placement: cluster
  viewBaseUrl: https://grafana.dummy
  navigationNodes:
    - label: Metrics
      navigationPath: ''
      viewUrl: ''
      externalLink: https://grafana.dummy
      order: 1

---
# Source: monitoring/templates/kyma-additions/destination-rule.yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: monitoring-prometheus
  namespace: default
spec:
  host: monitoring-prometheus.default.svc.cluster.local
  trafficPolicy:
    tls:
      mode: DISABLE
---
# Source: monitoring/charts/grafana/templates/policy.yaml
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: release-name-grafana-policy
spec:
  targets:
    - name: release-name-grafana
  peers:
    - mtls:
        mode: "PERMISSIVE"
---
# Source: monitoring/templates/kyma-additions/istio-telemetry-prometheus-svc-monitor-patch.yaml
# remove after fix https://github.com/istio/istio/issues/22801
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: istio-telemetry
  namespace: default
  labels:
    app: istio-telemetry
spec:
  selector:
    matchLabels:
      app: mixer
      istio: mixer
      operator.istio.io/component: Telemetry
  namespaceSelector:
    matchNames:
      - "istio-system"
  endpoints:
    - port: prometheus
      metricRelabelings:
      - sourceLabels: [ __name__ ]
        regex: ^(envoy_cluster_upstream_cx_active|envoy_cluster_upstream_cx_connect_fail|envoy_cluster_upstream_cx_rx_bytes_total|envoy_cluster_upstream_cx_total|envoy_cluster_upstream_cx_tx_bytes_total|envoy_server_hot_restart_epoch|go_goroutines|go_memstats_alloc_bytes|go_memstats_heap_alloc_bytes|go_memstats_heap_inuse_bytes|go_memstats_heap_sys_bytes|go_memstats_stack_inuse_bytes|grpc_io_server_completed_rpcs|grpc_io_server_server_latency_bucket|istio_build|istio_mcp_request_acks_total|istio_mcp_request_nacks_total|mixer_runtime_dispatch_duration_seconds_bucket|mixer_runtime_dispatches_total|process_cpu_seconds_total|process_max_fds|process_open_fds|process_resident_memory_bytes|process_start_time_seconds|process_virtual_memory_bytes|go_goroutines|go_memstats_alloc_bytes|go_memstats_heap_alloc_bytes|go_memstats_heap_inuse_bytes|go_memstats_heap_sys_bytes|go_memstats_stack_inuse_bytes|process_cpu_seconds_total|process_max_fds|process_open_fds|process_resident_memory_bytes|process_start_time_seconds|process_virtual_memory_bytes|go_goroutines|go_memstats_alloc_bytes|go_memstats_heap_alloc_bytes|go_memstats_heap_inuse_bytes|go_memstats_heap_sys_bytes|go_memstats_stack_inuse_bytes|istio_build|istio_mcp_request_acks_total|pilot_conflict_inbound_listener|pilot_conflict_outbound_listener_http_over_current_tcp|pilot_conflict_outbound_listener_tcp_over_current_http|pilot_conflict_outbound_listener_tcp_over_current_tcp|pilot_proxy_convergence_time_bucket|pilot_services|pilot_virt_services|pilot_xds_push_context_errors|pilot_total_xds_rejects|pilot_total_xds_internal_errors|pilot_xds_write_timeout|pilot_xds_lds_reject|pilot_xds_rds_reject|pilot_xds_push_timeout_failures|pilot_xds_eds_instances|pilot_xds_eds_reject|pilot_xds|pilot_xds_push_timeout|pilot_xds_push_errors|pilot_xds_cds_reject|pilot_xds_pushes|process_cpu_seconds_total|process_max_fds|process_open_fds|process_resident_memory_bytes|process_start_time_seconds|process_virtual_memory_bytes|galley_istio_authentication_meshpolicies|galley_istio_networking_destinationrules|galley_istio_networking_gateways|galley_istio_networking_virtualservices|galley_runtime_processor_events_processed_total|galley_runtime_processor_snapshot_events_total_bucket|galley_runtime_processor_snapshots_published_total|galley_runtime_state_type_instances_total|galley_runtime_strategy_on_change_total|galley_runtime_strategy_timer_max_time_reached_total|galley_runtime_strategy_timer_quiesce_reached_total|galley_runtime_strategy_timer_resets_total|galley_source_kube_dynamic_converter_failure_total|galley_source_kube_dynamic_converter_success_total|galley_source_kube_event_error_total|galley_source_kube_event_success_total|galley_validation_http_error|galley_validation_cert_key_update_errors|galley_validation_cert_key_updates|galley_validation_passed|go_goroutines|go_memstats_alloc_bytes|go_memstats_heap_alloc_bytes|go_memstats_heap_inuse_bytes|go_memstats_heap_sys_bytes|go_memstats_stack_inuse_bytes|istio_build|istio_mcp_clients_total|istio_mcp_request_acks_total|process_cpu_seconds_total|process_max_fds|process_open_fds|process_resident_memory_bytes|process_start_time_seconds|process_virtual_memory_bytes|citadel_secret_controller_secret_deleted_cert_count|citadel_secret_controller_svc_acc_deleted_cert_count|citadel_secret_controller_svc_acc_created_cert_count|citadel_server_authentication_failure_count|citadel_server_csr_count|citadel_secret_controller_csr_err_count|citadel_server_csr_parsing_err_count|citadel_server_success_cert_issuance_count|go_goroutines|go_memstats_alloc_bytes|go_memstats_heap_alloc_bytes|go_memstats_heap_inuse_bytes|go_memstats_heap_sys_bytes|go_memstats_stack_inuse_bytes|grpc_server_handled_total|grpc_server_handling_seconds_bucket|grpc_server_started_total|istio_build|process_cpu_seconds_total|process_max_fds|process_open_fds|process_resident_memory_bytes|process_start_time_seconds|process_virtual_memory_bytes|secret_deleted_cert_count|svc_acc_created_cert_count|svc_acc_deleted_cert_count|go_goroutines|go_memstats_alloc_bytes|go_memstats_heap_alloc_bytes|go_memstats_heap_inuse_bytes|go_memstats_heap_sys_bytes|go_memstats_stack_inuse_bytes|grpc_io_server_completed_rpcs|grpc_io_server_server_latency_bucket|istio_build|istio_mcp_request_acks_total|mixer_runtime_dispatch_duration_seconds_bucket|mixer_runtime_dispatches_total|process_cpu_seconds_total|process_max_fds|process_open_fds|process_resident_memory_bytes|process_start_time_seconds|process_virtual_memory_bytes|istio_request_bytes_bucket|istio_request_bytes_sum|istio_request_duration_seconds_bucket|istio_requests_total|istio_response_bytes_bucket|istio_response_bytes_sum|istio_tcp_received_bytes_total|istio_tcp_sent_bytes_total)$
        action: keep
---
# Source: monitoring/charts/grafana/templates/dashboards-json-configmap.yaml


---
# Source: monitoring/charts/grafana/templates/headless-service.yaml


---
# Source: monitoring/charts/grafana/templates/ingress.yaml


---
# Source: monitoring/charts/grafana/templates/poddisruptionbudget.yaml


---
# Source: monitoring/charts/grafana/templates/secret-env.yaml


---
# Source: monitoring/charts/grafana/templates/statefulset.yaml


---
# Source: monitoring/charts/kube-state-metrics/templates/pdb.yaml

---
# Source: monitoring/charts/kube-state-metrics/templates/psp-clusterrole.yaml


---
# Source: monitoring/charts/kube-state-metrics/templates/psp-clusterrolebinding.yaml


---
# Source: monitoring/charts/kube-state-metrics/templates/servicemonitor.yaml


---
# Source: monitoring/charts/kube-state-metrics/templates/stsdiscovery-role.yaml


---
# Source: monitoring/charts/kube-state-metrics/templates/stsdiscovery-rolebinding.yaml


---
# Source: monitoring/charts/prometheus-node-exporter/templates/endpoints.yaml


---
# Source: monitoring/charts/prometheus-node-exporter/templates/monitor.yaml


---
# Source: monitoring/templates/kyma-additions/alertmanager.config.yaml
global:
  resolve_timeout: 5m
route:
  receiver: 'null'
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  group_by: ['cluster','pod','job','alertname']
  # All alerts that do not match the following child routes
  # remain at the root node and are dispatched to 'default-receiver'
  routes:
  - receiver: 'null'
    match:
      alertname: Watchdog
  - receiver: "victorOps"
    continue: true # If set to `false`, it stops after the first matching.
    match_re:
            severity: critical
      
  - receiver: "slack"
    continue: true # If set to `false`, it stops after the first matching.
    match_re:
            severity: critical
      
receivers:
- name: 'null'
- name: "victorOps"
  victorops_configs:
  - api_key: "bbb"
    send_resolved: true
    api_url: https://alert.victorops.com/integrations/generic/20131114/alert/
    routing_key: "aaa"
    state_message: 'Alert: {{ .CommonLabels.alertname }}. Summary:{{ .CommonAnnotations.message }}. RawData: {{ .CommonLabels }}'
    entity_display_name: dummy
- name: "slack"
  slack_configs:
  - channel: "111111"
    send_resolved: true
    api_url: "1111"
    icon_emoji: ":ghost:"
    title: '{{ template "__subject" . }} (dummy)'
    title_link: 'https://console.dummy'
    text: '{{ range .Alerts }}<!channel>{{- "\n" -}}{{ .Annotations.message }}{{- "\n" -}}{{ end }}'

---
# Source: monitoring/charts/grafana/templates/kyma-additions/virtualservice.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: release-name-grafana
  labels:
    helm.sh/chart: grafana-5.2.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "7.0.6"
    app.kubernetes.io/managed-by: Tiller
spec:
  hosts:
  - grafana.dummy
  gateways:
  - kyma-system/kyma-gateway
  http:
  - match:
    - uri:
        regex: /.*
    route:
    - destination:
        port:
          number: 80
        host: release-name-grafana
